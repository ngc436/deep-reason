{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowEditDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset of factual knowledge based on KnowEdit.\n",
    "    Specifically selected from the QA validation slice from Mitchell et al.\n",
    "    Project page: http://nlp.cs.washington.edu/zeroshot/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str, size: typing.Optional[int] = None, config=None, *args, **kwargs):\n",
    "        data_dir = Path(data_dir)\n",
    "        zsre_loc = data_dir\n",
    "\n",
    "        if config is not None:\n",
    "            self.config = config\n",
    "        if config is not None and hasattr(config, 'max_length'):\n",
    "            self.max_length = config.max_length\n",
    "        else:\n",
    "            self.max_length = 40\n",
    "\n",
    "        # For Meta Training\n",
    "        if config is not None and hasattr(config, 'tokenizer_name'):\n",
    "            tok_name = (\n",
    "                config.tokenizer_name\n",
    "                if config.tokenizer_name is not None\n",
    "                else config.model.name\n",
    "            )\n",
    "            # tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)\n",
    "            tokenizer = getattr(transformers, config.tokenizer_class).from_pretrained(\n",
    "                tok_name, trust_remote_code=True\n",
    "            )\n",
    "            if isinstance(tokenizer, GPT2Tokenizer) or isinstance(tokenizer, GPT2TokenizerFast):\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                tokenizer.padding_side = 'left'\n",
    "                print('GPTTokenizer Detected, Set pad token id and left padding!!!')\n",
    "            elif isinstance(tokenizer, LlamaTokenizer):\n",
    "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "                tokenizer.padding_side = 'left'\n",
    "                print('LlamaTokenizer Detected, Set pad token id and left padding!!!')\n",
    "            if 'qwen' in config.model_name.lower():\n",
    "                tokenizer.eos_token='<|endoftext|>'\n",
    "                tokenizer.pad_token='<|endoftext|>'\n",
    "                tokenizer.unk_token='<|endoftext|>'\n",
    "                # tokenizer.padding_side = 'left'\n",
    "                # print('QwenTokenizer Detected, Set pad token id and left padding!!!')\n",
    "            self.tok = tokenizer\n",
    "\n",
    "        with open(zsre_loc, \"r\") as f:\n",
    "            raw = json.load(f)\n",
    "\n",
    "        data = []\n",
    "        for i, record in enumerate(raw):\n",
    "            data.append(\n",
    "                {\n",
    "                    \"subject\":record[\"subject\"] if \"subject\" in record else record[\"concept\"],\n",
    "                    \"prompt\": record[\"prompt\"] if \"prompt\" in record else record[\"text\"],\n",
    "                    \"target_new\": record[\"target_new\"] if \"target_new\" in record else record[\"labels\"],\n",
    "                    \"ground_truth\": record[\"ground_truth\"] if \"ground_truth\" in record else None,\n",
    "                    \"portability_r\": record[\"portability\"][\"Reasoning\"] if \"portability\" in record and \"Reasoning\" in record[\"portability\"] else None,\n",
    "                    \"portability_s\": record[\"portability\"][\"Subject_Aliasing\"] if \"portability\" in record and \"Subject_Aliasing\" in record[\"portability\"] else None,\n",
    "                    \"portability_l\":record[\"portability\"][\"Logical_Generalization\"] if \"portability\" in record and \"Logical_Generalization\" in record[\"portability\"] else None,\n",
    "                    \"locality_rs\": record[\"locality\"][\"Relation_Specificity\"] if \"Relation_Specificity\" in record[\"locality\"] else None,\n",
    "                    \"locality_f\": record[\"locality\"][\"Forgetfulness\"] if \"Forgetfulness\" in record[\"locality\"] else None\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if size is not None:\n",
    "            data = data[:size]\n",
    "        self._data = data\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def get_edit_labels(self, labels):\n",
    "        return labels.masked_fill(labels == self.tok.pad_token_id, -100)\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        src = [b[\"prompt\"] for b in batch]\n",
    "        trg = [b[\"target_new\"] for b in batch]\n",
    "        loc_data = [b[\"locality_rs\"] if b[\"locality_rs\"]!=None else b[\"locality_f\"] for b in batch]\n",
    "        loc=[l[0][\"prompt\"] if isinstance(l[0][\"prompt\"],str) else l[0][\"prompt\"][0] for l in loc_data]\n",
    "        loc_ans = [l[0][\"ground_truth\"][0] if isinstance(l[0][\"ground_truth\"][0],str) else l[0][\"ground_truth\"][0][0] for l in loc_data]\n",
    "\n",
    "        batches = {\n",
    "            f\"{k1}_{k2}\": v2\n",
    "            for k1, v1 in {\n",
    "                \"src\": src,\n",
    "                \"trg\": trg,\n",
    "            }.items()\n",
    "            for k2, v2 in self.tok(\n",
    "                v1,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "            ).items()\n",
    "        }\n",
    "\n",
    "        batches[\"raw\"] = batch\n",
    "\n",
    "        # edit_inner\n",
    "        edit_inner = {}\n",
    "        edit_inner[\"input_ids\"] = batches[\"src_input_ids\"]\n",
    "        edit_inner[\"attention_mask\"] = batches[\"src_attention_mask\"]\n",
    "        edit_labels = self.get_edit_labels(batches[\"trg_input_ids\"])\n",
    "\n",
    "        edit_inner[\"labels\"] = edit_labels\n",
    "\n",
    "        # loc\n",
    "        loc = dict(\n",
    "            self.tok(\n",
    "                loc,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        loc_ans = dict(\n",
    "            self.tok(\n",
    "                loc_ans,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "        )\n",
    "        loc[\"decoder_attention_mask\"] = loc_ans[\"attention_mask\"]\n",
    "        loc[\"labels\"] = self.get_edit_labels(loc_ans[\"input_ids\"])\n",
    "\n",
    "        # portability TODO\n",
    "\n",
    "        batch = {\n",
    "            \"edit_inner\": edit_inner,\n",
    "            \"loc\": loc,\n",
    "            \"raw\": batch,\n",
    "        }\n",
    "        return dict_to(batch, self.config.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
