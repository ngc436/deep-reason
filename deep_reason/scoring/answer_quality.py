from typing import List
from pydantic import BaseModel, Field
from deep_reason.utils import VLLMChatOpenAI
from deep_reason.chains import build_chain
from langchain_core.output_parsers import PydanticOutputParser
import os

question_prompt = """
You are an intelligent system capable of measuring the quality of an answer given by some Question Answering system.

You will be given a reference answer that was generated by context, an answer, and some context.

You need to measure the quality of the answer given the question and the context.

Evaluate based on:
1. Factual consistency with the context
2. Semantic similarity to the reference answer
3. Completeness of information
4. Presence of hallucinations or irrelevant information

Score from 0-10 where 10 is perfect alignment with reference answer and context.
"""

class AnswerQualityResult(BaseModel):
    explanation: str = Field(description="Detailed explanation of the answer quality assessment")
    score: float = Field(description="Numerical score from 0 to 10, 10 being the best", ge=0, le=10)

def calculate_answer_quality(generated_answer: List[str], real_answer: List[str], context: List[List[str]]) -> List[float]:
    # Initialize LLM with VLLM backend
    llm = VLLMChatOpenAI(
        model="/model",
        base_url=os.environ.get("OPENAI_API_BASE", "http://localhost:8000/v1"),
        api_key=os.environ.get("OPENAI_API_KEY", "sk-xxx"),
        temperature=0,
        max_tokens=1024
    )

    # Build evaluation chain
    system_template = question_prompt
    human_template = """\
Reference Answer: {real_answer}
Generated Answer: {generated_answer}
Context: {context}

{response_format_description}"""

    parser = PydanticOutputParser(pydantic_object=AnswerQualityResult)
    eval_chain = build_chain(llm, system_template, human_template, parser)

    results = []
    for gen, real, ctx in zip(generated_answer, real_answer, context):
        result = eval_chain.invoke({
            "generated_answer": gen,
            "real_answer": real,
            "context": "\n".join(ctx)
        })
        results.append(result.score)

    return results